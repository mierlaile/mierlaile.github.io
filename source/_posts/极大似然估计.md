---
title: 极大似然估计
date: 2018-08-22 16:28:26
tags:
  - 统计学
  - 机器学习
mathjax: true
---

本文主要解释了极大似然估计的概念和计算方法，并以计算朴素贝叶斯中的先验概率为实例。

<!-- more -->

> 本文引用了以下几篇文章，_对作者表示感谢
>
> https://zhuanlan.zhihu.com/p/32568242
>
> https://www.cnblogs.com/xing901022/p/8418894.html

#### 1.背景

> 一个袋子中有100个球，其中黑颜色球90个，白颜色球10个，此时随便取出一个球，这个球的颜色是什么？

大多数人肯定会猜测这个球是黑色的。因为黑球数量多，拿出的球是黑球的可能性最大。

这就是**极大似然估计（maximum likelihood estimation, MLE）** 的思维过程。极大似然估计也可以通俗地称为**最可能估计**。

#### 2.定义

在之前的介绍似然函数的文章中，已经写出了似然的定义，如下。
$$
\begin{equation}\begin{split}
L(\theta|data) &= P(data|\theta) = \prod_{i=1}^NP(x_i|\theta)\\
&data = (x_1,x_2,...,x_n)
\end{split}\end{equation}
$$
而极大似然估计要做的就是使$L(\theta|data)$的值最大，然后得到相应的$\theta$值，即：
$$
\hat\theta = \mathop{argmax}_\theta L(\theta|data)
$$
$\hat\theta$ 使得 $L(\theta)$ 的取值最大，那么$\hat\theta$ 是$\theta$ 的极大似然估计值。

#### 3.举例

以似然函数的文章中的抛硬币问题为例，H表示硬币正面朝上，概率为p(也就是似然中的参数)，T表示硬币背面朝上，概率为1-p。

先将硬币抛掷两次，如果两次硬币都是正面朝上，可以写出这样的似然函数：
$$
L(p|HH) = P(HH;p) = p \times p
$$
把这个函数画出来，如下图，是一个顶点在原点的抛物线，

![似然函数图像](https://blog-1252843561.cos.ap-beijing.myqcloud.com/20190109094848.png)

可以看到横轴p取得1，似然函数值最大。

此时我们再将硬币抛掷三次，硬币两次正面朝上，一次反面朝上，
$$
L(p|HHT) = P(HHT;p) = p\times p \times (1-p)
$$
函数如下图，

![似然函数图像](https://blog-1252843561.cos.ap-beijing.myqcloud.com/20190109095020.png)

此时横轴p取得0.66时，似然函数值最大。

可以看到这两次的实验结果并不符合正常抛硬币的主观经验，出现这个问题主要是因为实验次数过少。

继续以硬币为例，尝试通过计算得到极大似然估计的结果。

假设投掷硬币N次，正面n次，则似然函数可以表达为： 
$$
L(p|data) = P(data;p) = p^{n}\times(1-p)^{N-n}
$$
要求这个似然函数最大值，为了简化计算，我们将$ln$加入这个函数（由于对数函数属于单调递增函数，因此不会改变原函数的极值点），将幂转为系数($ln(a^b) = blna$)，将累乘转为累加（$ln(ab)=lna+lnb$）。所以可以将添加了对数的似然函数称为**对数似然函数**。
$$
lnL(p|data) = nlnp+(N-n)ln(1-p)
$$
接着对这个对数似然函数求导，并令导数为0，求极值。（一个函数在某一点的一阶导数为0是这个函数在该点取得极值点的必要条件）
$$
\frac{dlnL(p|data)}{dp} = \frac{n}{p} - \frac{N-n}{1-p} = 0
$$
故
$$
p = \frac{n}{N}
$$
根据极大似然估计，硬币正面朝上的概率，就是正面出现的概率除以总的投掷次数，换句话说，就是正面出现的频率 。

（题外话：极大似然估计是频率学派眼中估计概率的方式 。可是这样估计，如果我只投掷了1次，正面出现1次，按照最大似然估计我硬币正面的概率就是1，这是明显有问题的。这时贝叶斯学派站出来了，说我们觉得这样估计参数有问题，还应该考虑一些在做实验之前的经验，例如这枚硬币如果没被动过手脚，那么其正面的概率应该在0.5附近。贝叶斯学派认为应该把这些做实验之前的经验（先验概率）加入到参数估计的过程中，于是提出了最大后验估计。）

#### 4.总结计算方式

1. 构造似然函数$L(\theta)$
2. 取对数$lnL(\theta)$
3. 求导取零，计算极值
4. 解方程，得到$\theta$

以上是**解析法**对应的步骤，求得的结果是**解析解**。意思就是使用准确的公式去计算。

但有些实际问题会很复杂，求相应的解析解会非常困难，所以你可能会见到使用**数值法**去计算，求得的结果是**数值解**，也就是依据某些优化方法求得近似解，比如梯度下降法和牛顿法等等。具体步骤如下，与解析法部分相同：

1. 构造似然函数$L(\theta)$
2. 取对数$lnL(\theta)$
3. 梯度下降法、牛顿法等等
4. 得到近似解$\theta$

#### 5.补充一个实例，利用极大似然估计计算朴素贝叶斯法中的先验概率

这一点作为补充，没有接触过朴素贝叶斯的先不用看。

此处的朴素贝叶斯用于文档分类，文档共有K类，使用$c_k$表示文档类别。

这个问题要求解每个文档类别的先验概率，也就是求解参数$\theta_k = P(Y=c_k),k\epsilon\{1,2,...,K\}$。

给定N个文档，每个文档都有各自的类别，使用$y_i$表示第$i$个文档所属类别。
$$
L(\theta_k|data) = L(\theta_k|y_1,y_2,...,y_N) =P(y1,y2,...,y_N;\theta_k)   \\  =\prod_{i=1}^{N}P(y_i;\theta_k)=\prod_{i=1}^{N}P_{\theta_k}(y_i) =\prod_{i=1}^{N}(\sum_{k=1}^K\theta_kI(y_i=c_k))= \prod_{k=1}^{K}\theta_k^{N_k}
$$
最后的$N_k$表示N个文档中，属于K类的文档个数。

接着取对数，
$$
l(\theta_k) = lnL(\theta_k|data) = \sum_{k=1}^{K}N_kln\theta_k
$$
要求该函数的最大值，注意到约束条件$\sum_{k=1}^K\theta_k = 1$可以用拉格朗日乘子法，即 
$$
l(\theta_k,\lambda) = \sum_{k=1}^K{N_kln\theta_k} + \lambda(\sum_{k=1}^K\theta_k-1)
$$
求导就可以得到
$$
\frac{N_k}{\theta_k}+\lambda=0
$$
联立所有的k以及约束条件得到
$$
\theta_k = \frac{N_k}{N}
$$
完毕。







