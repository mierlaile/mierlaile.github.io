<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[似然函数]]></title>
    <url>%2F2018%2F08%2F22%2F%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[本文主要解释了似然的概念，似然和概率的区别和联系。 本文主要参考了以下链接，对其进行了部分简化和总结 https://baike.baidu.com/item/似然函数/6011241https://www.zhihu.com/question/54082000/answer/145495695https://www.cnblogs.com/zhsuiy/p/4822020.html 1. 似然与概率的区别似然 (likehood) 与概率 (probability) 在英语语境中是可以互换的。但是在统计学中，二者有截然不同的用法。 概率描述了已知参数时的随机变量的输出结果；似然则用来描述已知随机变量输出结果时，未知参数的可能取值。 例如，对于“一枚正反对称的硬币上抛十次”这种事件，我们可以问硬币落地时十次都是正面向上的“概率”是多少；而对于“一枚硬币上抛十次”，我们则可以问，这枚硬币正反面对称的“似然”程度是多少。 区别似然和概率的直接方法为，”谁谁谁的概率”中谁谁谁只能是事件，也就是，事件(发生)的概率是多少；而”谁谁谁的似然”中的谁谁谁只能是参数，比如说，参数等于某个值时的似然是多少。 2. 似然与概率的联系先看似然函数的定义。关于参数$\theta$的似然函数$L(\theta|data)$（在数值上）等于给定参数$\theta$后变量$data$的概率（两者的相等并不是说两个函数是同一个，只是数值上的相等，如果还不理解这个定义为什么是这样，可以先跳过往下看）： L(\theta|data) = P(data|\theta) = \prod_{i=1}^NP(x_i|\theta) \\ data = (x_1,x_2,...,x_n)存在两个疑问： 似然函数 (likelihood function) 为什么会写成条件概率的形式？ 给定的明明是$data$，为什么公式的右边，却变成了给定 $\theta$ 呢？ 常说的概率是指给定参数后，预测即将发生的事件的可能性。拿硬币这个例子来说，我们已知一枚均匀硬币的正反面概率分别是0.5($p = p_H = p_ T = 0.5$)，要预测抛两次硬币，正面都朝上的概率： H代表Head，表示正面朝上 也就是$data = (x_1,x_2) = (H,H)$ P(HH | p = 0.5) = \prod_{i=1}^2 P(x_i|p=0.5) = 0.5*0.5 = 0.25这种写法其实有点误导，后面的这个$p$其实是作为参数存在的（其实用$\theta$表示参数更好看一些，但是这里还要表示硬币正面朝上的概率，所以就先用$p$来做表示），而不是一个随机变量，因此不能算作是条件概率，更靠谱的写法应该是 $P(HH;p=0.5)$，分号;表示把参数隔开。平常求概率的时候是不写参数的，因为参数（硬币正面朝上的概率）是确定的，但这里是在讲解似然函数，参数是似然函数最终要求得的结果，所以这里就显式表明了参数的取值。比如，在其他参数取值下，可以写成$P(HH | p = 0.4) = 0.4*0.4 = 0.16$。 似然概率正好与这个过程相反，我们关注的量不再是事件的发生概率，而是已知发生了某些事件，我们希望知道参数应该是多少。 现在我们已经抛了两次硬币，并且知道了结果是两次正面朝上，这时候，我希望知道这枚硬币抛出去正面朝上的概率为0.5的概率是多少？正面朝上的概率为0.8的概率是多少？（这里的0.5和0.8就是之前提到的参数） 如果我们希望知道正面朝上概率为0.5的概率，这个东西就叫做似然函数，可以说成是对某一个参数的猜想（$p=0.5$）的概率，这样表示成(条件)概率就是 L(p=0.5|HH) = P(HH|p=0.5) = P(HH;p=0.5)为什么可以写成这样？我觉得可以这样来想： 似然函数本身也是一种概率，我们可以把$L(p=0.5|HH)$写成$P(p=0.5|HH)$； 而根据贝叶斯公式，$P(p=0.5|HH) = P(p=0.5,HH)/P(HH)$；既然$HH$是已经发生的事件，理所当然$P(HH) = 1$,所以： P(p=0.5|HH) = P(p=0.5,HH) = P(HH;p=0.5)右边的这个计算我们很熟悉了，就是已知正面朝上概率为0.5，求抛两次都是H的概率，即0.5*0.5=0.25。 所以，我们可以得到 L(p=0.5|HH) = P(HH|p=0.5) = 0.25这个0.25的意思是，在已知抛出两个正面的情况下，$p = 0.5$的概率等于0.25。 再算一下 L(p=0.6|HH) = P(HH|p=0.6) = 0.36把p从0~1的取值所得到的似然函数的曲线画出来得到这样一张图： 可以发现，$p = 1$的概率是最大的。 即$L(p = 1|HH) = 1$。 那么极大似然估计的问题也就好理解了（之后会有一篇单独文章）。 极大似然估计，就是在已知观测的数据的前提下，找到使得似然概率最大的参数值。 这就不难理解，在数据挖掘领域，许多求参数的方法最终都归结为极大似然估计的问题。 回到这个硬币的例子上来，在观测到HH的情况下，$p = 1$是最合理的（却未必符合真实情况，因为数据量太少的缘故）。 希望大家看到这里，可以明白似然函数为什么这样定义，还不明白就记住它吧，毕竟定义是定死的。 3. 似然函数遇到离散和连续数据两种情况3.1 $x_1,x_2,…,x_n$是离散数据上面关于硬币的例子就是离散型数据。此时可以具体给出$P_\theta(x_i)$的值，即代表在给定参数$\theta$下取得$x_i$的概率。 L(\theta|data) = P(data|\theta) = P(data;\theta) = P_\theta(data) = \prod_{i=1}^NP_\theta(x_i) \\ data = (x_1,x_2,...x_i,...,x_n)使用上式就可以求出给定参数$\theta$和$data$时，离散型随机变量对应的似然。 如果我们发现 L(\theta_1|data) = P_{\theta_1}(data) = \prod_{i=1}^NP_{\theta_1}(x_i) > \prod_{i=1}^NP_{\theta_2}(x_i) = P_{\theta_2}(data) = L(\theta_2|data)那么似然函数就反应出这样一个朴素推测：在参数$\theta_1$下随机向量$data$取到值$(x_1,x_2,…,x_n)$的可能性大于 在参数$\theta_2$下随机向量$data$取到值$(x_1,x_2,…,x_n)$的可能性。换句话说，我们更有理由相信(相对于$\theta_2$来说)$\theta_1$更有可能是真实值。这里的可能性由概率来刻画。 3.2 $x_1,x_2,…,x_n$是连续数据因为是连续型数据，所以无法给出$P_\theta(x_i)$的值或者说$P_\theta(x_i)$的值为零，而可以给出的是概率密度$f$，也就是一个区间内的概率。给定一个充分小$\epsilon &gt; 0$，那么随机变量$x_i$取值在$(x - \epsilon, x + \epsilon)$区间内的概率即为 P_{\theta}(x-\epsilon < x_i < x+\epsilon) = \int_{x-\epsilon}^{x+\epsilon}f(x_i|\theta)dx \approx 2\epsilon f(x_i|\theta) = 2\epsilon P(x_i|\theta) = 2\epsilon L(\theta|x_i)建立了似然函数和概率之间的关系，就可以像离散型变量一样，比较两个参数的似然，得到一个推测，如下所示。 L(\theta_1|data) \propto P_{\theta_1}(data) \propto \prod_{i=1}^NP_{\theta_1}(x-\epsilon]]></content>
      <tags>
        <tag>统计学</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[极大似然估计]]></title>
    <url>%2F2018%2F08%2F22%2F%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[本文主要解释了极大似然估计的概念和计算方法，并以计算朴素贝叶斯中的先验概率为实例。 本文引用了以下几篇文章，_对作者表示感谢 https://zhuanlan.zhihu.com/p/32568242 https://www.cnblogs.com/xing901022/p/8418894.html 1.背景 一个袋子中有100个球，其中黑颜色球90个，白颜色球10个，此时随便取出一个球，这个球的颜色是什么？ 大多数人肯定会猜测这个球是黑色的。因为黑球数量多，拿出的球是黑球的可能性最大。 这就是极大似然估计（maximum likelihood estimation, MLE） 的思维过程。极大似然估计也可以通俗地称为最可能估计。 2.定义在之前的介绍似然函数的文章中，已经写出了似然的定义，如下。 L(\theta|data) = P(data|\theta) = \prod_{i=1}^NP(x_i|\theta) \\ data = (x_1,x_2,...,x_n)而极大似然估计要做的就是使$L(\theta|data)$的值最大，然后得到相应的$\theta$值，即： \hat\theta = \mathop{argmax}_\theta L(\theta|data)$\hat\theta$ 使得 $L(\theta)$ 的取值最大，那么$\hat\theta$ 是$\theta$ 的极大似然估计值。 3.举例以上一篇文章中的抛硬币问题为例，H表示硬币正面朝上，概率为p(也就是似然中的参数)，T表示硬币背面朝上，概率为1-p。 先将硬币抛掷两次，如果两次硬币都是正面朝上，可以写出这样的似然函数： L(p|HH) = P(HH;p) = p * p把这个函数画出来，如下图，是一个顶点在原点的抛物线， 可以看到横轴p取得1，似然函数值最大。 此时我们再将硬币抛掷三次，硬币两次正面朝上，一次反面朝上， L(p|HHT) = P(HHT;p) = p*p*(1-p)函数如下图， 此时横轴p取得0.66时，似然函数值最大。 可以看到这两次的实验结果并不符合正常抛硬币的主观经验，出现这个问题主要是因为实验次数过少。 继续以硬币为例，尝试通过计算得到极大似然估计的结果。 假设投掷硬币N次，正面n次，则似然函数可以表达为： L(p|data) = P(data;p) = p^{n}*(1-p)^{N-n}要求这个似然函数最大值，为了简化计算，我们将$ln$加入这个函数（由于对数函数属于单调递增函数，因此不会改变原函数的极值点），将幂转为系数($ln(a^b) = blna$)，将累乘转为累加（$ln(ab)=lna+lnb$）。所以可以将添加了对数的似然函数称为对数似然函数。 lnL(p|data) = nlnp+(N-n)ln(1-p)接着对这个对数似然函数求导，并令导数为0，求极值。（一个函数在某一点的一阶导数为0是这个函数在该点取得极值点的必要条件） \frac{dlnL(p|data)}{dp} = \frac{n}{p} - \frac{N-n}{1-p} = 0故 p = \frac{n}{N}根据极大似然估计，硬币正面朝上的概率，就是正面出现的概率除以总的投掷次数，换句话说，就是正面出现的频率 。 （题外话：极大似然估计是频率学派眼中估计概率的方式 。可是这样估计，如果我只投掷了1次，正面出现1次，按照最大似然估计我硬币正面的概率就是1，这是明显有问题的。这时贝叶斯学派站出来了，说我们觉得这样估计参数有问题，还应该考虑一些在做实验之前的经验，例如这枚硬币如果没被动过手脚，那么其正面的概率应该在0.5附近。贝叶斯学派认为应该把这些做实验之前的经验（先验概率）加入到参数估计的过程中，于是提出了最大后验估计。） 4.总结计算方式 构造似然函数$L(\theta)$ 取对数$lnL(\theta)$ 求导取零，计算极值 解方程，得到$\theta$ 以上是解析法对应的步骤，求得的结果是解析解。意思就是使用准确的公式去计算。 但有些实际问题会很复杂，求相应的解析解会非常困难，所以你可能会见到使用数值法去计算，求得的结果是数值解，也就是依据某些优化方法求得近似解，比如梯度下降法和牛顿法等等。具体步骤如下，与解析法部分相同： 构造似然函数$L(\theta)$ 取对数$lnL(\theta)$ 梯度下降法、牛顿法等等 得到近似解$\theta$ 5.补充一个实例，利用极大似然估计计算朴素贝叶斯法中的先验概率这一点作为补充，没有接触过朴素贝叶斯的先不用看。 此处的朴素贝叶斯用于文档分类，文档共有K类，使用$c_k$表示文档类别。 这个问题要求解每个文档类别的先验概率，也就是求解参数$（）\theta_k = P（Y=c_k）,k\epsilon\{1,2,…,K\}$。 给定N个文档，每个文档都有各自的类别，使用$y_i$表示第$i$个文档所属类别。 L(\theta_k|data) = L(\theta_k|y_1,y_2,...,y_N) =P(y1,y2,...,y_N;\theta_k) \\ =\prod_{i=1}^{N}P(y_i;\theta_k)=\prod_{i=1}^{N}P_{\theta_k}(y_i) =\prod_{i=1}^{N}(\sum_{k=1}^K\theta_kI(y_i=c_k))= \prod_{k=1}^{K}\theta_k^{N_k}最后的$N_k$表示N个文档中，属于K类的文档个数。 接着取对数， l(\theta_k) = lnL(\theta_k|data) = \sum_{k=1}^{K}N_kln\theta_k要求该函数的最大值，注意到约束条件$\sum_{k=1}^K\theta_k = 1$可以用拉格朗日乘子法，即 l(\theta_k,\lambda) = \sum_{k=1}^K{N_kln\theta_k} + \lambda(\sum_{k=1}^K\theta_k-1)求导就可以得到 \frac{N_k}{\theta_k}+\lambda=0联立所有的k以及约束条件得到 \theta_k = \frac{N_k}{N}完毕。]]></content>
      <tags>
        <tag>统计学</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
</search>
